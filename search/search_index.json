{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Obsidian Notes \u00b6 Publish your public notes with MkDocs Hello World! \u00b6 The index.md in the /docs folder is the homepage you see here. The folders in /docs appear as the main sections on the navigation bar. The notes appear as pages within these sections. For example, Note 1 in Topic 1","title":"Obsidian Notes"},{"location":"#obsidian-notes","text":"Publish your public notes with MkDocs","title":"Obsidian Notes"},{"location":"#hello-world","text":"The index.md in the /docs folder is the homepage you see here. The folders in /docs appear as the main sections on the navigation bar. The notes appear as pages within these sections. For example, Note 1 in Topic 1","title":"Hello World!"},{"location":"Features/LaTeX%20Math%20Support/","text":"LaTeX Math Support \u00b6 LaTeX math is supported using MathJax. Inline math looks like \\(f(x) = x^2\\) . The input for this is $f(x) = x^2$ . Use $...$ . For a block of math, use $$...$$ on separate lines $$ F(x) = \\int^a_b \\frac{1}{2}x^4 $$ gives \\[ F(x) = \\int^a_b \\frac{1}{2}x^4 \\]","title":"LaTeX Math Support"},{"location":"Features/LaTeX%20Math%20Support/#latex-math-support","text":"LaTeX math is supported using MathJax. Inline math looks like \\(f(x) = x^2\\) . The input for this is $f(x) = x^2$ . Use $...$ . For a block of math, use $$...$$ on separate lines $$ F(x) = \\int^a_b \\frac{1}{2}x^4 $$ gives \\[ F(x) = \\int^a_b \\frac{1}{2}x^4 \\]","title":"LaTeX Math Support"},{"location":"Features/Mermaid%20Diagrams/","text":"Mermaid diagrams \u00b6 Here's the example from MkDocs Material documentation : graph LR A[Start] --> B{Error?}; B -->|Yes| C[Hmm...]; C --> D[Debug]; D --> B; B ---->|No| E[Yay!];","title":"Mermaid diagrams"},{"location":"Features/Mermaid%20Diagrams/#mermaid-diagrams","text":"Here's the example from MkDocs Material documentation : graph LR A[Start] --> B{Error?}; B -->|Yes| C[Hmm...]; C --> D[Debug]; D --> B; B ---->|No| E[Yay!];","title":"Mermaid diagrams"},{"location":"Features/Text%20Formatting/","text":"Text Formatting \u00b6 You can have lists like this first second third Or checklist lists to Get things done Also, get highlights and strikethroughs as above (similar to Obsidian). More formatting options for your webpage here . (but not compatible with Obsidian)","title":"Text Formatting"},{"location":"Features/Text%20Formatting/#text-formatting","text":"You can have lists like this first second third Or checklist lists to Get things done Also, get highlights and strikethroughs as above (similar to Obsidian). More formatting options for your webpage here . (but not compatible with Obsidian)","title":"Text Formatting"},{"location":"Git/Switching%20Development%20Contexts%20with%20Git/","text":"Switching Development Contexts with Git \u00b6 Context \u00b6 On my daily work I have to switch context while in the middle of a modification many times, or I just have to work on two different versions in parallel. Working with git there are many ways to save your work and come back later, but I have found all of them too cumbersome in the process to resume work. I do not need this, I already have stash to save my work! \u00b6 Yeah you can stash your work and checkout a new branch but stash is not very user friendly and if you spend too much time in your new task you will have a hard time remembering wich stash element was the good one unless you have a really strict naming policy while stashig things ... which, to he honest most of us do not have in our daily work, so let alone when we are in a hurry. Well, I really do NOT need this cause I use WIP commits ... \u00b6 Sure, you can commit your progress with some kind of wip label and checkout a new branch but that would mostly result in you having to move changes around between commits to tidy up your changes in small and meaningfull commits before committing the final work ... and still there is the problem with checkout \u00b6 Both of the above systems rely in save your work somehow and checkout a new branch to star working in the new urgent task. That usually works fine but ... what if your new branch is an old maintenance branch and your project layout or your build system had changed ... ? Well your IDE will probably freak out and you will end having to deal with a bunch of errors in your indexer, openned files which do not exist anymore ... you probably know what I mean How worktree works \u00b6 worktree is a git command. It allows you to create more than one working tree for each repository. Normally each repo has one main working tree (maybe zero on bare repos but they are out of scope here), and zero or more linked working trees. worktree will allow you to checkout a new or already existing branch in a new folder, that way you can switch between them with zero effort. ... but I already can clone the repo multiple times, so no big deal! \u00b6 Ok, yes. You already can clone a git repo on different folders and you would have something very similar to the scenario above. In fact that was my former approach to the problem. So what makes learning to use worktree worthy? \u00b6 Well, first of all make a new repo clone takes disk space, sometimes a lot of disk space. As worktrees are not new clones but just linked workingtrees of a main repo, they share most of the \"administration\" info with your main working tree. Another improvement of worktree over multiple clones is in fact a \"limitation\" that worktree impose on us but that I have found quite useful to avoid errors. worktree does NOT allow us to have more than one workingtree pointing to the same branch at the same time. This will help us to avoid headaches having more than one way to modify the content of a branch. Finally, being a git command worktree provides useful commands to list , add and delete workingtrees and so having them under control from a central point. Some IDEs already have support for working trees also. Use cases \u00b6 I have found worktree command most useful when: - Working on two different PRs and have to switch between then to address comments - I have to solve bugs or test something on old maintenance versions Brief commands reference \u00b6 Create a new working tree and a new branch from the current point. \u00b6 git worktree add -b <new_branch_name> <new_folder_path> Create a new working tree from an existing branch \u00b6 git worktree add <new_folder_path> <existing_branch_name> List all the existing working trees \u00b6 git worktree list Links and more info \u00b6 Official docs for worktree : https://git-scm.com/docs/git-worktree/2.35.0 An StackOverflow thread with some additional info regarding commands Bonus track: How I layout my workingtrees \u00b6 Create a new folder with the project name $> mkdir awesomeProject Clone the main (or master if you still live in the old good days) in its own folder $> cd awesomeProject awesomeProject$> git clone git@github.com:jlojosnegros/awesome-project.git main From there you could create your new workingtrees to work on your features using sibling folders awesomeProject$> cd main awesomeProject/main[main {origin/main}]$> git worktree list ~/awesomeProject/main d4f9c34 [main] awesomeProject/main[main {origin/main}]$> git worktree add -b newAwesomeFeature ../new-awesome-feature Preparing worktree (new branch 'newAwesomeFeature') HEAD is now at d4f9c34 My first and last commit awesomeProject/main[main {origin/main}]$> git worktree list ~/awesomeProject/main d4f9c34 [main] ~/awesomeProject/new-awesome-feature d4f9c34 [newAwesomeFeature] awesomeProject/main[main {origin/main}]$> cd ../new-awesome-feature awesomeProject/new-awesome-feature[newAwesomeFeature {L}]$> TL;DR \u00b6 TBD","title":"Switching Development Contexts with Git"},{"location":"Git/Switching%20Development%20Contexts%20with%20Git/#switching-development-contexts-with-git","text":"","title":"Switching Development Contexts with Git"},{"location":"Git/Switching%20Development%20Contexts%20with%20Git/#context","text":"On my daily work I have to switch context while in the middle of a modification many times, or I just have to work on two different versions in parallel. Working with git there are many ways to save your work and come back later, but I have found all of them too cumbersome in the process to resume work.","title":"Context"},{"location":"Git/Switching%20Development%20Contexts%20with%20Git/#i-do-not-need-this-i-already-have-stash-to-save-my-work","text":"Yeah you can stash your work and checkout a new branch but stash is not very user friendly and if you spend too much time in your new task you will have a hard time remembering wich stash element was the good one unless you have a really strict naming policy while stashig things ... which, to he honest most of us do not have in our daily work, so let alone when we are in a hurry.","title":"I do not need this, I already have stash to save my work!"},{"location":"Git/Switching%20Development%20Contexts%20with%20Git/#well-i-really-do-not-need-this-cause-i-use-wip-commits","text":"Sure, you can commit your progress with some kind of wip label and checkout a new branch but that would mostly result in you having to move changes around between commits to tidy up your changes in small and meaningfull commits before committing the final work","title":"Well, I really do NOT need this cause I use WIP commits ..."},{"location":"Git/Switching%20Development%20Contexts%20with%20Git/#and-still-there-is-the-problem-with-checkout","text":"Both of the above systems rely in save your work somehow and checkout a new branch to star working in the new urgent task. That usually works fine but ... what if your new branch is an old maintenance branch and your project layout or your build system had changed ... ? Well your IDE will probably freak out and you will end having to deal with a bunch of errors in your indexer, openned files which do not exist anymore ... you probably know what I mean","title":"... and still there is the problem with checkout"},{"location":"Git/Switching%20Development%20Contexts%20with%20Git/#how-worktree-works","text":"worktree is a git command. It allows you to create more than one working tree for each repository. Normally each repo has one main working tree (maybe zero on bare repos but they are out of scope here), and zero or more linked working trees. worktree will allow you to checkout a new or already existing branch in a new folder, that way you can switch between them with zero effort.","title":"How worktree works"},{"location":"Git/Switching%20Development%20Contexts%20with%20Git/#but-i-already-can-clone-the-repo-multiple-times-so-no-big-deal","text":"Ok, yes. You already can clone a git repo on different folders and you would have something very similar to the scenario above. In fact that was my former approach to the problem.","title":"... but I already can clone the repo multiple times, so no big deal!"},{"location":"Git/Switching%20Development%20Contexts%20with%20Git/#so-what-makes-learning-to-use-worktree-worthy","text":"Well, first of all make a new repo clone takes disk space, sometimes a lot of disk space. As worktrees are not new clones but just linked workingtrees of a main repo, they share most of the \"administration\" info with your main working tree. Another improvement of worktree over multiple clones is in fact a \"limitation\" that worktree impose on us but that I have found quite useful to avoid errors. worktree does NOT allow us to have more than one workingtree pointing to the same branch at the same time. This will help us to avoid headaches having more than one way to modify the content of a branch. Finally, being a git command worktree provides useful commands to list , add and delete workingtrees and so having them under control from a central point. Some IDEs already have support for working trees also.","title":"So what makes learning to use worktree worthy?"},{"location":"Git/Switching%20Development%20Contexts%20with%20Git/#use-cases","text":"I have found worktree command most useful when: - Working on two different PRs and have to switch between then to address comments - I have to solve bugs or test something on old maintenance versions","title":"Use cases"},{"location":"Git/Switching%20Development%20Contexts%20with%20Git/#brief-commands-reference","text":"","title":"Brief commands reference"},{"location":"Git/Switching%20Development%20Contexts%20with%20Git/#create-a-new-working-tree-and-a-new-branch-from-the-current-point","text":"git worktree add -b <new_branch_name> <new_folder_path>","title":"Create a new working tree and a new branch from the current point."},{"location":"Git/Switching%20Development%20Contexts%20with%20Git/#create-a-new-working-tree-from-an-existing-branch","text":"git worktree add <new_folder_path> <existing_branch_name>","title":"Create a new working tree from an existing branch"},{"location":"Git/Switching%20Development%20Contexts%20with%20Git/#list-all-the-existing-working-trees","text":"git worktree list","title":"List all the existing working trees"},{"location":"Git/Switching%20Development%20Contexts%20with%20Git/#links-and-more-info","text":"Official docs for worktree : https://git-scm.com/docs/git-worktree/2.35.0 An StackOverflow thread with some additional info regarding commands","title":"Links and more info"},{"location":"Git/Switching%20Development%20Contexts%20with%20Git/#bonus-track-how-i-layout-my-workingtrees","text":"Create a new folder with the project name $> mkdir awesomeProject Clone the main (or master if you still live in the old good days) in its own folder $> cd awesomeProject awesomeProject$> git clone git@github.com:jlojosnegros/awesome-project.git main From there you could create your new workingtrees to work on your features using sibling folders awesomeProject$> cd main awesomeProject/main[main {origin/main}]$> git worktree list ~/awesomeProject/main d4f9c34 [main] awesomeProject/main[main {origin/main}]$> git worktree add -b newAwesomeFeature ../new-awesome-feature Preparing worktree (new branch 'newAwesomeFeature') HEAD is now at d4f9c34 My first and last commit awesomeProject/main[main {origin/main}]$> git worktree list ~/awesomeProject/main d4f9c34 [main] ~/awesomeProject/new-awesome-feature d4f9c34 [newAwesomeFeature] awesomeProject/main[main {origin/main}]$> cd ../new-awesome-feature awesomeProject/new-awesome-feature[newAwesomeFeature {L}]$>","title":"Bonus track: How I layout my workingtrees"},{"location":"Git/Switching%20Development%20Contexts%20with%20Git/#tldr","text":"TBD","title":"TL;DR"},{"location":"Topic%201/Note%201/","text":"Note 1 \u00b6 Example: link to Mermaid Diagrams under Features","title":"Note 1"},{"location":"Topic%201/Note%201/#note-1","text":"Example: link to Mermaid Diagrams under Features","title":"Note 1"},{"location":"Topic%201/Note%202/","text":"Note 2 \u00b6","title":"Note 2"},{"location":"Topic%201/Note%202/#note-2","text":"","title":"Note 2"},{"location":"kubernetes/architecture/Arquitectura%20de%20Kubernetes/","text":"Arquitectura de Kubernetes \u00b6 kubernetes \u00b6 ![[KubernetesArchitecture.png.png]] Componentes \u00b6 Control Plane \u00b6 Aqui es donde est\u00e1n los elementos que manejan el comportamiento del cluster pero que no ejecutan nada de las aplicaciones de los containers. Componentes \u00b6 etcd API Server #api-server Scheduler Controller Manager The Nodes ( workers ) \u00b6 Aqui es donde se ejecutan los containers de la aplicaci\u00f3n. Componentes \u00b6 Kubelet Kubernetes Service Proxy (kube-proxy) Conainer Runtime (Docker, rkt ... ) Add-on Components \u00b6 A parte de los componentes del Control Plane y de los nodos se necesitan unas cuantas cosas para que todo funcione bien Kubernetes DNS Server Dashboard Ingress Controller Heapster Container Network Interface network plugin Como se comunican \u00b6 Todos se comunican con el API server. El API server es el \u00fanico que se comunica con la base de datos. Todos modifican el estado de la base de datos hablando con el API server. La comunicaci\u00f3n es siempre comenzada por los otros elementos salvo en algunos casos especiales donde el API server es quien comienza la conexion. - Cuando usamos kubectl para obtener los logs - Cuando usamos kubectl attach - Cuando usamos kubectl port-forward Ejecutando multiples instancias. \u00b6 Todos los elementos que tienen los Worker Nodes tienen que correr en el mismo worker node. ( supongo que en cada nodo tienen que estar todos los elementos) Sin embargo los elementos del control plane pueden ser ejecutados en distintos servidores. Se pueden ejecutar multiples instancias de los elementos del Control plane para tener HA. Sin embargo: - Etcd y API server pueden tener varias instancias funcionando en paralelo - Scheduler y Controller Manager solo pueden tener una instancia funcionando al mismo tiempo mientras que las dem\u00e1s est\u00e1n en stand-by Como se ejecutan los componentes \u00b6 Todos los componentes del control plane y el kube-proxy pueden ejecutarse en el sistema directamente o pueden ejecutarse como pods en el master node . El unico que tiene que ejecutarse como un componente de sistema siempre es Kubelet Como se usan los componentes \u00b6 Como utiliza K8s ETCD \u00b6 Es el \u00fanico sitio donde se guardan los manifiestos de todos los componentes que se crean en el cluster. Es una base de datos clave-valor distribuida, lo que permite poder tener multiples instancias para tener HA El unico que se comunica con el etcd es el API Server. Esto tiene unas ventajas - Se puede cambiar la base de datos con facilidad adaptando el API server. - Puede implementarse un sistema de locking optimista - Se puede hacer validacion de los elementos antes de guardarlos o de recuperarlos. Como se guardan las cosas en etcd \u00b6 Hay dos versiones en uso v2 y v3 ( aunque seguramente ahora la v2 ya no se use porque la v3 tiene mejor performance.) En v2 - Las claves son jerarquicas y estan guardadas en arbol, por lo que puede pensarse en la clave de una entrada como en el nombre completo de un fichero en un arbol de directorios. - Por tanto cada clave es: - O bien un directorio que a su vez contiene otras claves - O bien una clave regular que contiene un valor. En v3 - No soporta el concepto de directorios per se, pero como las claves siguen siendo organizadas de manera jer\u00e1rquica y se pueden poner / , podemos seguir pensando en ellas como el nombre completo de un fichero dentro de un filesystem. Todos los datos que kubernetes guarda en etcd se guardan debajo de /registry title: Listado de entradas en etcd V2 ```bash $ etcdctl ls /registry ``` V3: Las entradas cuya clave empieza por un prefijo determinado. ```bash $ etcdtl get /registry --prefix=true ``` Salida ```bash /registry/configmaps /registry/daemonsets /registry/deployments /registry/events /registry/namespaces /registry/pods ... ``` title : pods en el namespace default collapse: true ```bash $ etcdctl ls /registry/pods/default /registry/pods/default/kubia-159041347-xk0vc /registry/pods/default/kubia-159041347-wt6ga /registry/pods/default/kubia-159041347-hp2o5 ``` y esto es lo que se guarda de una entrada en particular ```bash $ etcdctl get /registry/pods/default/kubia-159041347-wt6ga {\"kind\":\"Pod\",\"apiVersion\":\"v1\",\"metadata\":{\"name\":\"kubia-159041347-wt6ga\", \"generateName\":\"kubia-159041347-\",\"namespace\":\"default\",\"selfLink\":... ``` Como puede verse es un documento en formato json. Consistencia en ETCD cuando hay un cluster. \u00b6 Como podemos tener multiples instancias de etcd en un cluster tenemos que asegurar que hay consistencia en el estado que muestran todas. Para esto se utiliza el [[Algoritmo RAFT]] Es un algoritmo de consenso, de manera que para transicionar a un nuevo estado, mas de la mitad de los nodos tienen que estar de acuerdo. En este algoritmo se asegura que una determinada instancia de etcd o bien muestra el ultimo estado al que se ha llegado o bien muestra uno de los antiguos estados a los que se llego por consenso. Si se produce un split brain, si una de las particiones tiene la mayoria de los nodos evidentemente la otra no los tendr\u00e1. Por tanto la primera particion si que podr\u00e1 transicionar de estado porque tienen la mayoria, mientras que la otra no podr\u00e1 modificar el estado del cluster, qued\u00e1ndose en un estado v\u00e1lido., Cuando la situacion se solucione los nodos que estaban en un estado antiguo, pero valido, podr\u00e1n recuperarse y hacer catch-up. Debido a la necesidad de quorum es mejor siempre tener un numero impar de instancias de etcd. Teniendo un numero par no aumentamos el numero de fallos de instancias a los que somos resilientes, pero si que aumentamos la posibilidad de fallo, ya que a mas instancias en el cluster mas posibilidades de que alguna falle. Normalmente los clusters mas grandes de etcd tienen CINCO o SIETE instancias, lo que permite manejar sin problemas el fallo de DOS o TRES instancias, lo que suele ser suficiente en la mayoria de los casos. Que hace el API server \u00b6 Intro \u00b6 Es el componente que todos los demas componentes, incluido el kubectl utilizan para poder interactuar con el sistema. Provee un interface CRUD sobre un API Rest Guarda todos los datos en ETCD Provee consistencia en los datos que se guardan Chequea los datos que los demas componentes quieren guardar en etcd antes de guardarlos para evitar estados inconsistentes del sistema Implementa un Optimistic Locking system Dado que podemos tener multiples instancias de API server ejecutandose de manera concurrente tenemos que evitar que los cambios puedan sobreescribir otros cambios que se est\u00e9n haciendo en ese momento 'kubectl' es uno de los clientes del API server. Cuando creamos un nuevo recurso con un fichero `yaml` lo que hace es enviar un HTTP POST con el fichero en formato json y esto es lo que pasa en el API Server ![[APIServerHTTPPOST.png]] Stages \u00b6 Authentication \u00b6 Lo primero que se hace es autenticar al cliente que envia la peticion. Se envia la peticion a una lista de plugins de autenticacion que esten configurados en el API server, pasando por todos hasta que uno de ellos sea capaz de determinar quien es el cliente que envia la peticion. Authorization \u00b6 Una vez que sabemos quien es el cliente que pide la accion y que podemos confiar en el, tenemos que ver si dicho cliente tiene autorizacion para poder hacer dicha accion sobre el recurso que esta pidiendo hacerla. Para esto estan los plugins de autorizacion. La peticion va pasando por los plugins hasta que uno de ellos dice que el cliente puede realizar al accion sobre el recurso pedido. Admision Control \u00b6 Las operaciones de Creacion, Modificacion y Borrado de recursos pasan por una fase adicional, la de Control de Admision. En esta fase la peticion pasas por todos los plugins . Estos plugins pueden modificar la peticion - A\u00f1adiendo valores por defecto que no existen en el recurso de la peticion - Cambiando valores del recursos que exceden limites Incluso pueden modificar recursos relacionados que no est\u00e9n en la peticion Tambien pueden validad y rechazar la peticion por el contenido que tiene. title: Ejemplos de Admission Control Plugins. - `AlwaysPullImages` -> Sobreescribe el `imagePullPolicy` de los Pods y lo pone a `Always` - `ServiceAccount` -> Aplica un `ServiceAccount` por defecto a todos los Pods que no definen uno. - `NamespaceLifecycle` -> Evita que se puedan crear Pods en namespaces que est\u00e1n en proceso de borrado o que no existen. - `ResourceQuota` -> Se asegura que los Pods de un determinado namespace solo puedan utilizar unas cantidades de CPU y memoria determinadas How API Server Notifies clients. \u00b6 El API server permite que los demas componentes pidan ser notificados cuando un recurso es Creado Modificado o Borrado, de manera que pueda hacer lo que sea que tenga que hacer en respuesta a esta operacion. El Cliente abre una conexion HTTP al API server y a trav\u00e9s de dicha conexion el cliente recive un stream de moficiaciones. Cada vez que uno de los objetos observados cambia se le envia al cliente la nueva version del objeto. Este mecanismo lo usa por ejemplo el kubectl cuando pedimos que nos haga un watch de los pods kubectl get pods -o yaml --watch Tambien lo utiliza el Scheduler Entendiendo el funcionamiento del Scheduler \u00b6 Controllers ejecutandose en el Controller Manager. \u00b6 Intro \u00b6 Hasta ahora tenemos en el control plane: - ETCD: que solo guarda los manifiestos. - API Server: que es un interface para que los demas puedan guardar cosas en la base de datos y ser notificados - Scheduler: Que se encarga de asingar nodos a los nuevos pods. Pero no tenemos nadie que haga nada para mantener el estado del cluster conciliado con el estado de los manifiestos guardados en la base de datos. Esto es lo que hace el Controller Manager, o para ser mas exactlos la lista de controllers que se ejecutan en el. Todos los controller se al formar parte del Control Plane, se ejecutan en el Master Node Controllers en general \u00b6 Actualmente ( no tengo claro que eso sea cierto ahora mismo) un solo proceso Controller Manager combina dentro de el varios controllers distintos que realizan distintas labores. Eventualmente cada uno de estos controllers ser\u00e1 separado en un proceso distinto para que sea mas sencillo cambiarlos. Lista de Controllers: - Replication Manager -- Un controller para ReplicationController resources - ReplicaSet Controller - DaemonSet Controller - Job Controller - Deployment Controller - StatefulSet Controller - Node Controller - Service Controller - Endpoints Controller - Namespace Controller - PersistentVolumeController - Others ... que cachondo. Basicamente hay un Controller por cada uno de los recursos de kubernetes. Los recursos son una declaracion de cual queremos que sea el estado de un cluster. Los Controllers realizan el trabajo de intentar conciliar el estado actual del cluster con el que queremos que sea. En general los controllers tienen un \"reconciliation loop\" donde intentan hacer esta reconciliacion del estado creando, modificando o borrando determinados recursos. [!note]- En cada recurso el spec es donde ponemos el estado esperado mientras que los controllers escriben en el status el estado actual. Utilizan el sistema de notificacion del API server para saber cuando se modifican los recursos a los que atienden. Tambien tienen una operacion \"re-list\" que ejecutan periodicamente por si acaso han perdido alguna notificacion. Los Controllers no hablan nunca unos con otros. Los Controllers no saben ni que existen otros controllers. Solo hablan con el API server nada mas. Cada uno hace su parte y esperan que todo funcione correctamente. Algunos controllers \u00b6 Replication Manager \u00b6 Basicamente espera ser notificado de los Pods y los [[Kubernetes ReplicationController|ReplicationController]] que se van creando, destruyendo o modificando para comprobar si lo que se ha puesto en los [[Kubernetes ReplicationController|ReplicationController]] se sigue cumpliendo. En el caso de que el n\u00famero de pods no sea el esperado realiza las operaciones adecuadas para poder llegar al estado deseado. Si necesita crear nuevos pods utiliza el spec.template del [[Kubernetes ReplicationController|ReplicationController]] para enviar un HTTP POST al API Server con el nuevo manifiesto para que se desencadene la creacion de un nuevo pod. El Replication Manager hace su trabajo solo modificando los manifiestos de los recursos mediante el API server como hacen todos los demas controllers ReplicaSet Controller, DaemonSet Controller y Job Controller \u00b6 Los tres hacen basicamente lo mismo que el Replication Manager, solo que usando distintos recursos... [[Kubernetes ReplicaSet]], [[Kubernetes DaemonSet]] y [[Kubernetes Job]] Deployment Controller \u00b6 Gestiona las actualizaciones de los [[Kubernetes Deployment|Deployment]]. Hace un roll-out del deployment cuando se modifica. Basicamente crea un nuevo [[Kubernetes ReplicaSet|Replicaset]] y despues realiza una escalacion tanto del antiguo como del nuevo [[Kubernetes ReplicaSet|Replicaset]] basandose en la estrategia definida en el [[Kubernetes Deployment|Deployment]] hasta que todos los viejos Pods han desaparecido y han sido reemplazados por los nuevos. StatefulSet Controller \u00b6 Como los demas Controllers el StatefulSet Controller crea y destruye pods de acuerdo con la especificacion de los [[Kubernetes StatefulSet|StatefulSet]]. Adem\u00e1s es el \u00fanico que tambien maneja los [[Kubernetes PersistentVolumeClaims| PersistentVolumeClaims]] para cada instancia de Pod Node Controller \u00b6 Maneja los [[Kubernetes Node Resources|Node Resources]] que describen los workers de un cluster. Entre otras cosas mantiene una lista actualizada de los nodos del cluster monitorizando las maquinas que se a\u00f1aden o quitan. Tambien monitoriza la salud de los nodos y elimina los Pods de los nodos que no pueden ser alcanzados. Service Controller \u00b6 Cuando un [[Kubernetes Service]] de tipo LoadBalancer se crea se pide un LoadBalancer al sistema para poder hacer el Servicio visible desde el exterior. Este controler es el encargado de realizar la operacion de pedir el recurso y de liberarlo cuando no es necesario. Endpoints Controller \u00b6 Los [[Kubernetes Service]] no son enlazados directamente a los Pods, si no que contienen una lista de los endpotins (IP + Puerto), lista que es actualizada de acuerdo con el pod selector del [[Kubernetes Service]] El Endpoints Controller es el encargado de mantener la lista de endpoints constantemente actualizada con las IPs y puertos de los Pods que matchean el label selector ![[KubernetesEndpointsManager.png]] Namespace Controller \u00b6 Cuando un namespace es borrado todos los recursos que pertenezcan a dicho namespace deben ser borrados. Esto es lo que hace el controller. PersistentVolume Controller \u00b6 Cuando un usuario crea un [[Kubernetes PersistentVolumeClaims]] kubernetes tiene que encontrar el [[Kubernetes PersistentVolume]] al que poder enlazarlo. Esto es justamente lo que hace este controller. Cuando se crea un nuevo [[Kubernetes PersistentVolumeClaims]] este controller intenta encontrar el mejor match para el. - Busca el [[Kubernetes PersistentVolume]] mas peque\u00f1o que tenga el modo de acceso adecuado y que tenga una capacidad suficiente para el claim. Para esto mantiene una lista ordenada de [[Kubernetes PersistentVolume]] para cada uno de los modos de acceso por orden ascendente de capacidad ( asi que solo tiene que devolver el primero de la lista.) Cuando el [[Kubernetes PersistentVolumeClaims]] es borrado el [[Kubernetes PersistentVolume]] es reclamado por el controller de acuerdo con la politica de reclaim ... ( puede quedarse como est\u00e1, ser borrado, ser vaciado...) Kubelet \u00b6 Intro \u00b6 Al contrario que los controllers tango kubelet como kube-proxy se ejecutan en los worker nodes, que es donde se ejecutan los containers de las aplicaciones. Responsabilidades \u00b6 Kubelet es el responsable de todo lo que sucede en un worker node. Lo primero que tiene que hacer es registrar el nodo en el cluster, para eso crea un [[Kubernetes Node Resources]] usando el API Server. A partir de ese momento monitoriza todos los Pods para ver cuales son asignados por el Scheduler para ser ejecutados en su nodo, y cuando esto sucede habla con el Container Runtime que haya configurado para que se ejecuten los containers del Pod. Tambi\u00e9n monitoriza constantemente los containers de los Pods que est\u00e1n ejecutandose en su Nodo para poder reportar su estatus, eventos y consumo de recursos al API Server. Kubelet tambien es el responsable de ejecutar los liveness probes, rearrancar los containers cuando la probe falle y por \u00faltimo ser quien borre los contenedores cuando el Pod sea borrado. Ejecutando Pods staticos sin el API Server. \u00b6 Normalmente kubelet obtiene los manifiestos de los pods que tiene que ejecutar del API server, pero tambien puede ejecutar otros Pods obteniendo los manifiestos de un directorio del Nodo en el que se ejecuta. Esto se utiliza principalmente para poder ejecutar componentes del Control Plane como Pods en lugar de como procesos del master node. Kubernetes Service Proxy \u00b6 Los Worker Nodes tambien ejecutan el kube-proxy. El \u00fanico cometido de kube-proxy es asegurarse que cuando llamamos a uno de los [[Kubernetes Service]] la llamada termina en uno de los Pods que est\u00e1 dando soporte a dicho servicio. Cuando hay mas de un Pod dando soporte a dicho servicio se realiza un load-balancing. Se llama kube-proxy porque en la primera de las implementaciones era un proceso en espacio de usuario que hab\u00eda modificado las [[IP tables]] para que las peticiones a los servicios fueran redirigidas a \u00e9l y luego fuese el quien las reenviase a los Pods adecuados. Todas las conexiones pasaban por \u00e9l. Ahora hay una implementaci\u00f3n mucho mejor en performance que simplemente modifica la configuracion de las [[IP tables]] para que las conexiones vayan a parar al Pod directamente sin pasar por el proceso kube-proxy , por lo que ya no es un proxy estrictamente hablando, pero el nombre se le ha quedado. La diferencia principal entre los dos modos es que en el antiguo al tener que pasar todos los mensajes por el kube-proxy todos tenian que pasar por el espacio de usuario, lo que era una putada para el performance. Ademas el antiguo hac\u00eda un round robin entre los pods de un servicio mientras que el nuevo lo hace de manera aleatoria. Kubernetes Add-ons \u00b6 Hay algunos otros componentes que, sin ser extrictamente necesarios, permiten realizar acciones interesantes en el cluster. Son los add-ons. Estos componentes se instalan en el sistema como Pods, enviando sus manifiestos al API server como cualquier otro pod. Algunos de ellos se instalan mediante [[Kubernetes Deployment|Deployment]] u otro tipo de recurso de k8s title: Algunos add-ons Podemos ver como en Minikube el Ingress Controller y el Dashboard son instalados mediante un [[Kubernetes ReplicationController|ReplicationController]] ```bash $ kubectl get rc -n kube-system NAME DESIRED CURRENT READY AGE default-http-backend 1 1 1 6d kubernetes-dashboard 1 1 1 6d nginx-ingress-controller 1 1 1 6d ``` Mientras que el DNS es un [[Kubernetes Deployment|Deployment]] ```bash $ kubectl get deploy -n kube-system NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE kube-dns 1 1 1 1 6d Como funciona el DNS server \u00b6 Todos los pods en un cluster estan configurados para utilizar el DNS interno por defecto. El DNS se expone mediante un servicio kube-dns para poder asi mover los pods dentro del cluster sin problema. La direccion IP del servicio esta configurada como el nameserver en el fichero /etc/resolv.conf dentro de cada container instalado en el cluster. kube-dns utiliza el mecanismo de notificaciones del API server para observar cambios en los [[Kubernetes Service]] y en los [[Kubernetes Endpoint|Endpoints]] y actualiza sus tablas internas con cada uno de los cambios. Durante el momento en el que se actualiza un [[Kubernetes Service]] o un [[Kubernetes Endpoint]] hasta que el kube-dns recibe la notificacion y actualiza sus registros puede que haya informaci\u00f3n erronea en el DNS Ejemplo de como funciona todo junto \u00b6 Todo kubernetes esta formado por peque\u00f1os elementos muy poco acoplados con una buena separacion de responsabilidades ( cada uno hace una cosa y solo una puta cosa) y esa es la raz\u00f3n por la que todo funciona bien Vamos a ver como funcionan todas las piezas en armon\u00eda para la creacion de un Pod. Normalmente no se crea un Pod desde cero, de modo que vamos a ver como se realiza la creacion de un Pod, pero partiendo del manifiesto de un Deployment. Antes siquiera de empezar ya tenemos la siguiente figura, donde cada uno de los componentes est\u00e1 escuchando las notificaciones adecuadas del API server respecto a los recursos que le son interesantes ( normalmente solo uno o un par a lo sumo) ![[KubernetesWatchingResources.png]] Bien, una vez que kubectl en via mediante un HTTP POST el manifiesto del deployment al API Server esta es la cadena de acontecimientos que se desata. ![[KubernetesPodCreationChainAction.png]] Se pueden ver todos los eventos que van sucediendo en el cluster mediante el siguiente comando de kubectl kubectl get events --watch Entendiendo que pasa cuando se ejecuta un POD \u00b6 Cuando ejecutamos un Pod en uno de los nodos no solo se ejecutan los containers que tenemos definidos en el pod, si no que ademas se ejecuta otro pod. Supongamos que solo tenemos un container en el pod para hacerlo mas sencillo. $ kubectl run nginx --image=nginx deployment \"nginx\" created Si vamos al nodo en el que se est\u00e1 ejecutando el pod podemos ver los siguientes contenedores ejecutandose. docker@minikubeVM:~$ docker ps CONTAINER ID IMAGE COMMAND CREATED c917a6f3c3f7 nginx \"nginx -g 'daemon off\" 4 seconds ago 98b8bf797174 gcr.io/.../pause:3.0 \"/pause\" 7 seconds ago Podemos ver que hay un contenedor adicional ejecutandose - Dicho contenedor no est\u00e1 ejecutando nada porque esta en pause - Ademas podemos ver que se ha creado antes que el contenedor que teniamos definido en el pod Ese contenedor es parte de la infraestructura - Todos los contenedores de un mismo pod comparten algunos linux namespaces como por ejemplo el espacio de red ... - Este contenedor es el que contiene todos los linux namespaces que son compartidos por todos los contenedores de un mismo pod. ![[KubernetesPodInfraContainer.png]] Todos los contenedores del mismo pod tienen que compartir los mismos linux namespaces aunque se reinicien, pero eso es posible porque estos est\u00e1n en este container, de manera que aunque los dem\u00e1s se reinicien los namespaces se mantienen. El ciclo de vida de este contenedor esta ligado intimamente al del pod. Si por cualquier motivo este container resultase parado, kubelet volver\u00eda a recrearlo y despu\u00e9s recrear\u00eda todos los demas containers del pod para poder asignarles los linux namespaces del nuevo container. Interpod networking \u00b6 Cada pod tiene un IP unica Todos los pods pueden comunicarse entre si mediante conexiones \"planas\" es decir sin NAT Kubernetes no es quien configura esto, es el administrador quien tiene que conseguirlo. Kubernetes solo pone el requisito. La red tiene que estar configurada de tal manera que la IP que un Pod ve de si mismo sea la misma que los dem\u00e1s pueden ver de \u00e9l. Todos los containers deben de poder comunicarse entre ellos independientemente de si estan siendo ejecutados en el mismo o en distintos nodos.","title":"Arquitectura de Kubernetes"},{"location":"kubernetes/architecture/Arquitectura%20de%20Kubernetes/#arquitectura-de-kubernetes","text":"","title":"Arquitectura de Kubernetes"},{"location":"kubernetes/architecture/Arquitectura%20de%20Kubernetes/#kubernetes","text":"![[KubernetesArchitecture.png.png]]","title":"kubernetes"},{"location":"kubernetes/architecture/Arquitectura%20de%20Kubernetes/#componentes","text":"","title":"Componentes"},{"location":"kubernetes/architecture/Arquitectura%20de%20Kubernetes/#control-plane","text":"Aqui es donde est\u00e1n los elementos que manejan el comportamiento del cluster pero que no ejecutan nada de las aplicaciones de los containers.","title":"Control Plane"},{"location":"kubernetes/architecture/Arquitectura%20de%20Kubernetes/#componentes_1","text":"etcd API Server #api-server Scheduler Controller Manager","title":"Componentes"},{"location":"kubernetes/architecture/Arquitectura%20de%20Kubernetes/#the-nodes-workers","text":"Aqui es donde se ejecutan los containers de la aplicaci\u00f3n.","title":"The Nodes ( workers )"},{"location":"kubernetes/architecture/Arquitectura%20de%20Kubernetes/#componentes_2","text":"Kubelet Kubernetes Service Proxy (kube-proxy) Conainer Runtime (Docker, rkt ... )","title":"Componentes"},{"location":"kubernetes/architecture/Arquitectura%20de%20Kubernetes/#add-on-components","text":"A parte de los componentes del Control Plane y de los nodos se necesitan unas cuantas cosas para que todo funcione bien Kubernetes DNS Server Dashboard Ingress Controller Heapster Container Network Interface network plugin","title":"Add-on Components"},{"location":"kubernetes/architecture/Arquitectura%20de%20Kubernetes/#como-se-comunican","text":"Todos se comunican con el API server. El API server es el \u00fanico que se comunica con la base de datos. Todos modifican el estado de la base de datos hablando con el API server. La comunicaci\u00f3n es siempre comenzada por los otros elementos salvo en algunos casos especiales donde el API server es quien comienza la conexion. - Cuando usamos kubectl para obtener los logs - Cuando usamos kubectl attach - Cuando usamos kubectl port-forward","title":"Como se comunican"},{"location":"kubernetes/architecture/Arquitectura%20de%20Kubernetes/#ejecutando-multiples-instancias","text":"Todos los elementos que tienen los Worker Nodes tienen que correr en el mismo worker node. ( supongo que en cada nodo tienen que estar todos los elementos) Sin embargo los elementos del control plane pueden ser ejecutados en distintos servidores. Se pueden ejecutar multiples instancias de los elementos del Control plane para tener HA. Sin embargo: - Etcd y API server pueden tener varias instancias funcionando en paralelo - Scheduler y Controller Manager solo pueden tener una instancia funcionando al mismo tiempo mientras que las dem\u00e1s est\u00e1n en stand-by","title":"Ejecutando multiples instancias."},{"location":"kubernetes/architecture/Arquitectura%20de%20Kubernetes/#como-se-ejecutan-los-componentes","text":"Todos los componentes del control plane y el kube-proxy pueden ejecutarse en el sistema directamente o pueden ejecutarse como pods en el master node . El unico que tiene que ejecutarse como un componente de sistema siempre es Kubelet","title":"Como se ejecutan los componentes"},{"location":"kubernetes/architecture/Arquitectura%20de%20Kubernetes/#como-se-usan-los-componentes","text":"","title":"Como se usan los componentes"},{"location":"kubernetes/architecture/Arquitectura%20de%20Kubernetes/#como-utiliza-k8s-etcd","text":"Es el \u00fanico sitio donde se guardan los manifiestos de todos los componentes que se crean en el cluster. Es una base de datos clave-valor distribuida, lo que permite poder tener multiples instancias para tener HA El unico que se comunica con el etcd es el API Server. Esto tiene unas ventajas - Se puede cambiar la base de datos con facilidad adaptando el API server. - Puede implementarse un sistema de locking optimista - Se puede hacer validacion de los elementos antes de guardarlos o de recuperarlos.","title":"Como utiliza K8s ETCD"},{"location":"kubernetes/architecture/Arquitectura%20de%20Kubernetes/#como-se-guardan-las-cosas-en-etcd","text":"Hay dos versiones en uso v2 y v3 ( aunque seguramente ahora la v2 ya no se use porque la v3 tiene mejor performance.) En v2 - Las claves son jerarquicas y estan guardadas en arbol, por lo que puede pensarse en la clave de una entrada como en el nombre completo de un fichero en un arbol de directorios. - Por tanto cada clave es: - O bien un directorio que a su vez contiene otras claves - O bien una clave regular que contiene un valor. En v3 - No soporta el concepto de directorios per se, pero como las claves siguen siendo organizadas de manera jer\u00e1rquica y se pueden poner / , podemos seguir pensando en ellas como el nombre completo de un fichero dentro de un filesystem. Todos los datos que kubernetes guarda en etcd se guardan debajo de /registry title: Listado de entradas en etcd V2 ```bash $ etcdctl ls /registry ``` V3: Las entradas cuya clave empieza por un prefijo determinado. ```bash $ etcdtl get /registry --prefix=true ``` Salida ```bash /registry/configmaps /registry/daemonsets /registry/deployments /registry/events /registry/namespaces /registry/pods ... ``` title : pods en el namespace default collapse: true ```bash $ etcdctl ls /registry/pods/default /registry/pods/default/kubia-159041347-xk0vc /registry/pods/default/kubia-159041347-wt6ga /registry/pods/default/kubia-159041347-hp2o5 ``` y esto es lo que se guarda de una entrada en particular ```bash $ etcdctl get /registry/pods/default/kubia-159041347-wt6ga {\"kind\":\"Pod\",\"apiVersion\":\"v1\",\"metadata\":{\"name\":\"kubia-159041347-wt6ga\", \"generateName\":\"kubia-159041347-\",\"namespace\":\"default\",\"selfLink\":... ``` Como puede verse es un documento en formato json.","title":"Como se guardan las cosas en etcd"},{"location":"kubernetes/architecture/Arquitectura%20de%20Kubernetes/#consistencia-en-etcd-cuando-hay-un-cluster","text":"Como podemos tener multiples instancias de etcd en un cluster tenemos que asegurar que hay consistencia en el estado que muestran todas. Para esto se utiliza el [[Algoritmo RAFT]] Es un algoritmo de consenso, de manera que para transicionar a un nuevo estado, mas de la mitad de los nodos tienen que estar de acuerdo. En este algoritmo se asegura que una determinada instancia de etcd o bien muestra el ultimo estado al que se ha llegado o bien muestra uno de los antiguos estados a los que se llego por consenso. Si se produce un split brain, si una de las particiones tiene la mayoria de los nodos evidentemente la otra no los tendr\u00e1. Por tanto la primera particion si que podr\u00e1 transicionar de estado porque tienen la mayoria, mientras que la otra no podr\u00e1 modificar el estado del cluster, qued\u00e1ndose en un estado v\u00e1lido., Cuando la situacion se solucione los nodos que estaban en un estado antiguo, pero valido, podr\u00e1n recuperarse y hacer catch-up. Debido a la necesidad de quorum es mejor siempre tener un numero impar de instancias de etcd. Teniendo un numero par no aumentamos el numero de fallos de instancias a los que somos resilientes, pero si que aumentamos la posibilidad de fallo, ya que a mas instancias en el cluster mas posibilidades de que alguna falle. Normalmente los clusters mas grandes de etcd tienen CINCO o SIETE instancias, lo que permite manejar sin problemas el fallo de DOS o TRES instancias, lo que suele ser suficiente en la mayoria de los casos.","title":"Consistencia en ETCD cuando hay un cluster."},{"location":"kubernetes/architecture/Arquitectura%20de%20Kubernetes/#que-hace-el-api-server","text":"","title":"Que hace el API server"},{"location":"kubernetes/architecture/Arquitectura%20de%20Kubernetes/#intro","text":"Es el componente que todos los demas componentes, incluido el kubectl utilizan para poder interactuar con el sistema. Provee un interface CRUD sobre un API Rest Guarda todos los datos en ETCD Provee consistencia en los datos que se guardan Chequea los datos que los demas componentes quieren guardar en etcd antes de guardarlos para evitar estados inconsistentes del sistema Implementa un Optimistic Locking system Dado que podemos tener multiples instancias de API server ejecutandose de manera concurrente tenemos que evitar que los cambios puedan sobreescribir otros cambios que se est\u00e9n haciendo en ese momento 'kubectl' es uno de los clientes del API server. Cuando creamos un nuevo recurso con un fichero `yaml` lo que hace es enviar un HTTP POST con el fichero en formato json y esto es lo que pasa en el API Server ![[APIServerHTTPPOST.png]]","title":"Intro"},{"location":"kubernetes/architecture/Arquitectura%20de%20Kubernetes/#stages","text":"","title":"Stages"},{"location":"kubernetes/architecture/Arquitectura%20de%20Kubernetes/#authentication","text":"Lo primero que se hace es autenticar al cliente que envia la peticion. Se envia la peticion a una lista de plugins de autenticacion que esten configurados en el API server, pasando por todos hasta que uno de ellos sea capaz de determinar quien es el cliente que envia la peticion.","title":"Authentication"},{"location":"kubernetes/architecture/Arquitectura%20de%20Kubernetes/#authorization","text":"Una vez que sabemos quien es el cliente que pide la accion y que podemos confiar en el, tenemos que ver si dicho cliente tiene autorizacion para poder hacer dicha accion sobre el recurso que esta pidiendo hacerla. Para esto estan los plugins de autorizacion. La peticion va pasando por los plugins hasta que uno de ellos dice que el cliente puede realizar al accion sobre el recurso pedido.","title":"Authorization"},{"location":"kubernetes/architecture/Arquitectura%20de%20Kubernetes/#admision-control","text":"Las operaciones de Creacion, Modificacion y Borrado de recursos pasan por una fase adicional, la de Control de Admision. En esta fase la peticion pasas por todos los plugins . Estos plugins pueden modificar la peticion - A\u00f1adiendo valores por defecto que no existen en el recurso de la peticion - Cambiando valores del recursos que exceden limites Incluso pueden modificar recursos relacionados que no est\u00e9n en la peticion Tambien pueden validad y rechazar la peticion por el contenido que tiene. title: Ejemplos de Admission Control Plugins. - `AlwaysPullImages` -> Sobreescribe el `imagePullPolicy` de los Pods y lo pone a `Always` - `ServiceAccount` -> Aplica un `ServiceAccount` por defecto a todos los Pods que no definen uno. - `NamespaceLifecycle` -> Evita que se puedan crear Pods en namespaces que est\u00e1n en proceso de borrado o que no existen. - `ResourceQuota` -> Se asegura que los Pods de un determinado namespace solo puedan utilizar unas cantidades de CPU y memoria determinadas","title":"Admision Control"},{"location":"kubernetes/architecture/Arquitectura%20de%20Kubernetes/#how-api-server-notifies-clients","text":"El API server permite que los demas componentes pidan ser notificados cuando un recurso es Creado Modificado o Borrado, de manera que pueda hacer lo que sea que tenga que hacer en respuesta a esta operacion. El Cliente abre una conexion HTTP al API server y a trav\u00e9s de dicha conexion el cliente recive un stream de moficiaciones. Cada vez que uno de los objetos observados cambia se le envia al cliente la nueva version del objeto. Este mecanismo lo usa por ejemplo el kubectl cuando pedimos que nos haga un watch de los pods kubectl get pods -o yaml --watch Tambien lo utiliza el Scheduler","title":"How API Server Notifies clients."},{"location":"kubernetes/architecture/Arquitectura%20de%20Kubernetes/#entendiendo-el-funcionamiento-del-scheduler","text":"","title":"Entendiendo el funcionamiento del Scheduler"},{"location":"kubernetes/architecture/Arquitectura%20de%20Kubernetes/#controllers-ejecutandose-en-el-controller-manager","text":"","title":"Controllers ejecutandose en el Controller Manager."},{"location":"kubernetes/architecture/Arquitectura%20de%20Kubernetes/#intro_1","text":"Hasta ahora tenemos en el control plane: - ETCD: que solo guarda los manifiestos. - API Server: que es un interface para que los demas puedan guardar cosas en la base de datos y ser notificados - Scheduler: Que se encarga de asingar nodos a los nuevos pods. Pero no tenemos nadie que haga nada para mantener el estado del cluster conciliado con el estado de los manifiestos guardados en la base de datos. Esto es lo que hace el Controller Manager, o para ser mas exactlos la lista de controllers que se ejecutan en el. Todos los controller se al formar parte del Control Plane, se ejecutan en el Master Node","title":"Intro"},{"location":"kubernetes/architecture/Arquitectura%20de%20Kubernetes/#controllers-en-general","text":"Actualmente ( no tengo claro que eso sea cierto ahora mismo) un solo proceso Controller Manager combina dentro de el varios controllers distintos que realizan distintas labores. Eventualmente cada uno de estos controllers ser\u00e1 separado en un proceso distinto para que sea mas sencillo cambiarlos. Lista de Controllers: - Replication Manager -- Un controller para ReplicationController resources - ReplicaSet Controller - DaemonSet Controller - Job Controller - Deployment Controller - StatefulSet Controller - Node Controller - Service Controller - Endpoints Controller - Namespace Controller - PersistentVolumeController - Others ... que cachondo. Basicamente hay un Controller por cada uno de los recursos de kubernetes. Los recursos son una declaracion de cual queremos que sea el estado de un cluster. Los Controllers realizan el trabajo de intentar conciliar el estado actual del cluster con el que queremos que sea. En general los controllers tienen un \"reconciliation loop\" donde intentan hacer esta reconciliacion del estado creando, modificando o borrando determinados recursos. [!note]- En cada recurso el spec es donde ponemos el estado esperado mientras que los controllers escriben en el status el estado actual. Utilizan el sistema de notificacion del API server para saber cuando se modifican los recursos a los que atienden. Tambien tienen una operacion \"re-list\" que ejecutan periodicamente por si acaso han perdido alguna notificacion. Los Controllers no hablan nunca unos con otros. Los Controllers no saben ni que existen otros controllers. Solo hablan con el API server nada mas. Cada uno hace su parte y esperan que todo funcione correctamente.","title":"Controllers en general"},{"location":"kubernetes/architecture/Arquitectura%20de%20Kubernetes/#algunos-controllers","text":"","title":"Algunos controllers"},{"location":"kubernetes/architecture/Arquitectura%20de%20Kubernetes/#replication-manager","text":"Basicamente espera ser notificado de los Pods y los [[Kubernetes ReplicationController|ReplicationController]] que se van creando, destruyendo o modificando para comprobar si lo que se ha puesto en los [[Kubernetes ReplicationController|ReplicationController]] se sigue cumpliendo. En el caso de que el n\u00famero de pods no sea el esperado realiza las operaciones adecuadas para poder llegar al estado deseado. Si necesita crear nuevos pods utiliza el spec.template del [[Kubernetes ReplicationController|ReplicationController]] para enviar un HTTP POST al API Server con el nuevo manifiesto para que se desencadene la creacion de un nuevo pod. El Replication Manager hace su trabajo solo modificando los manifiestos de los recursos mediante el API server como hacen todos los demas controllers","title":"Replication Manager"},{"location":"kubernetes/architecture/Arquitectura%20de%20Kubernetes/#replicaset-controller-daemonset-controller-y-job-controller","text":"Los tres hacen basicamente lo mismo que el Replication Manager, solo que usando distintos recursos... [[Kubernetes ReplicaSet]], [[Kubernetes DaemonSet]] y [[Kubernetes Job]]","title":"ReplicaSet Controller, DaemonSet Controller y Job Controller"},{"location":"kubernetes/architecture/Arquitectura%20de%20Kubernetes/#deployment-controller","text":"Gestiona las actualizaciones de los [[Kubernetes Deployment|Deployment]]. Hace un roll-out del deployment cuando se modifica. Basicamente crea un nuevo [[Kubernetes ReplicaSet|Replicaset]] y despues realiza una escalacion tanto del antiguo como del nuevo [[Kubernetes ReplicaSet|Replicaset]] basandose en la estrategia definida en el [[Kubernetes Deployment|Deployment]] hasta que todos los viejos Pods han desaparecido y han sido reemplazados por los nuevos.","title":"Deployment Controller"},{"location":"kubernetes/architecture/Arquitectura%20de%20Kubernetes/#statefulset-controller","text":"Como los demas Controllers el StatefulSet Controller crea y destruye pods de acuerdo con la especificacion de los [[Kubernetes StatefulSet|StatefulSet]]. Adem\u00e1s es el \u00fanico que tambien maneja los [[Kubernetes PersistentVolumeClaims| PersistentVolumeClaims]] para cada instancia de Pod","title":"StatefulSet Controller"},{"location":"kubernetes/architecture/Arquitectura%20de%20Kubernetes/#node-controller","text":"Maneja los [[Kubernetes Node Resources|Node Resources]] que describen los workers de un cluster. Entre otras cosas mantiene una lista actualizada de los nodos del cluster monitorizando las maquinas que se a\u00f1aden o quitan. Tambien monitoriza la salud de los nodos y elimina los Pods de los nodos que no pueden ser alcanzados.","title":"Node Controller"},{"location":"kubernetes/architecture/Arquitectura%20de%20Kubernetes/#service-controller","text":"Cuando un [[Kubernetes Service]] de tipo LoadBalancer se crea se pide un LoadBalancer al sistema para poder hacer el Servicio visible desde el exterior. Este controler es el encargado de realizar la operacion de pedir el recurso y de liberarlo cuando no es necesario.","title":"Service Controller"},{"location":"kubernetes/architecture/Arquitectura%20de%20Kubernetes/#endpoints-controller","text":"Los [[Kubernetes Service]] no son enlazados directamente a los Pods, si no que contienen una lista de los endpotins (IP + Puerto), lista que es actualizada de acuerdo con el pod selector del [[Kubernetes Service]] El Endpoints Controller es el encargado de mantener la lista de endpoints constantemente actualizada con las IPs y puertos de los Pods que matchean el label selector ![[KubernetesEndpointsManager.png]]","title":"Endpoints Controller"},{"location":"kubernetes/architecture/Arquitectura%20de%20Kubernetes/#namespace-controller","text":"Cuando un namespace es borrado todos los recursos que pertenezcan a dicho namespace deben ser borrados. Esto es lo que hace el controller.","title":"Namespace Controller"},{"location":"kubernetes/architecture/Arquitectura%20de%20Kubernetes/#persistentvolume-controller","text":"Cuando un usuario crea un [[Kubernetes PersistentVolumeClaims]] kubernetes tiene que encontrar el [[Kubernetes PersistentVolume]] al que poder enlazarlo. Esto es justamente lo que hace este controller. Cuando se crea un nuevo [[Kubernetes PersistentVolumeClaims]] este controller intenta encontrar el mejor match para el. - Busca el [[Kubernetes PersistentVolume]] mas peque\u00f1o que tenga el modo de acceso adecuado y que tenga una capacidad suficiente para el claim. Para esto mantiene una lista ordenada de [[Kubernetes PersistentVolume]] para cada uno de los modos de acceso por orden ascendente de capacidad ( asi que solo tiene que devolver el primero de la lista.) Cuando el [[Kubernetes PersistentVolumeClaims]] es borrado el [[Kubernetes PersistentVolume]] es reclamado por el controller de acuerdo con la politica de reclaim ... ( puede quedarse como est\u00e1, ser borrado, ser vaciado...)","title":"PersistentVolume Controller"},{"location":"kubernetes/architecture/Arquitectura%20de%20Kubernetes/#kubelet","text":"","title":"Kubelet"},{"location":"kubernetes/architecture/Arquitectura%20de%20Kubernetes/#intro_2","text":"Al contrario que los controllers tango kubelet como kube-proxy se ejecutan en los worker nodes, que es donde se ejecutan los containers de las aplicaciones.","title":"Intro"},{"location":"kubernetes/architecture/Arquitectura%20de%20Kubernetes/#responsabilidades","text":"Kubelet es el responsable de todo lo que sucede en un worker node. Lo primero que tiene que hacer es registrar el nodo en el cluster, para eso crea un [[Kubernetes Node Resources]] usando el API Server. A partir de ese momento monitoriza todos los Pods para ver cuales son asignados por el Scheduler para ser ejecutados en su nodo, y cuando esto sucede habla con el Container Runtime que haya configurado para que se ejecuten los containers del Pod. Tambi\u00e9n monitoriza constantemente los containers de los Pods que est\u00e1n ejecutandose en su Nodo para poder reportar su estatus, eventos y consumo de recursos al API Server. Kubelet tambien es el responsable de ejecutar los liveness probes, rearrancar los containers cuando la probe falle y por \u00faltimo ser quien borre los contenedores cuando el Pod sea borrado.","title":"Responsabilidades"},{"location":"kubernetes/architecture/Arquitectura%20de%20Kubernetes/#ejecutando-pods-staticos-sin-el-api-server","text":"Normalmente kubelet obtiene los manifiestos de los pods que tiene que ejecutar del API server, pero tambien puede ejecutar otros Pods obteniendo los manifiestos de un directorio del Nodo en el que se ejecuta. Esto se utiliza principalmente para poder ejecutar componentes del Control Plane como Pods en lugar de como procesos del master node.","title":"Ejecutando Pods staticos sin el API Server."},{"location":"kubernetes/architecture/Arquitectura%20de%20Kubernetes/#kubernetes-service-proxy","text":"Los Worker Nodes tambien ejecutan el kube-proxy. El \u00fanico cometido de kube-proxy es asegurarse que cuando llamamos a uno de los [[Kubernetes Service]] la llamada termina en uno de los Pods que est\u00e1 dando soporte a dicho servicio. Cuando hay mas de un Pod dando soporte a dicho servicio se realiza un load-balancing. Se llama kube-proxy porque en la primera de las implementaciones era un proceso en espacio de usuario que hab\u00eda modificado las [[IP tables]] para que las peticiones a los servicios fueran redirigidas a \u00e9l y luego fuese el quien las reenviase a los Pods adecuados. Todas las conexiones pasaban por \u00e9l. Ahora hay una implementaci\u00f3n mucho mejor en performance que simplemente modifica la configuracion de las [[IP tables]] para que las conexiones vayan a parar al Pod directamente sin pasar por el proceso kube-proxy , por lo que ya no es un proxy estrictamente hablando, pero el nombre se le ha quedado. La diferencia principal entre los dos modos es que en el antiguo al tener que pasar todos los mensajes por el kube-proxy todos tenian que pasar por el espacio de usuario, lo que era una putada para el performance. Ademas el antiguo hac\u00eda un round robin entre los pods de un servicio mientras que el nuevo lo hace de manera aleatoria.","title":"Kubernetes Service Proxy"},{"location":"kubernetes/architecture/Arquitectura%20de%20Kubernetes/#kubernetes-add-ons","text":"Hay algunos otros componentes que, sin ser extrictamente necesarios, permiten realizar acciones interesantes en el cluster. Son los add-ons. Estos componentes se instalan en el sistema como Pods, enviando sus manifiestos al API server como cualquier otro pod. Algunos de ellos se instalan mediante [[Kubernetes Deployment|Deployment]] u otro tipo de recurso de k8s title: Algunos add-ons Podemos ver como en Minikube el Ingress Controller y el Dashboard son instalados mediante un [[Kubernetes ReplicationController|ReplicationController]] ```bash $ kubectl get rc -n kube-system NAME DESIRED CURRENT READY AGE default-http-backend 1 1 1 6d kubernetes-dashboard 1 1 1 6d nginx-ingress-controller 1 1 1 6d ``` Mientras que el DNS es un [[Kubernetes Deployment|Deployment]] ```bash $ kubectl get deploy -n kube-system NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE kube-dns 1 1 1 1 6d","title":"Kubernetes Add-ons"},{"location":"kubernetes/architecture/Arquitectura%20de%20Kubernetes/#como-funciona-el-dns-server","text":"Todos los pods en un cluster estan configurados para utilizar el DNS interno por defecto. El DNS se expone mediante un servicio kube-dns para poder asi mover los pods dentro del cluster sin problema. La direccion IP del servicio esta configurada como el nameserver en el fichero /etc/resolv.conf dentro de cada container instalado en el cluster. kube-dns utiliza el mecanismo de notificaciones del API server para observar cambios en los [[Kubernetes Service]] y en los [[Kubernetes Endpoint|Endpoints]] y actualiza sus tablas internas con cada uno de los cambios. Durante el momento en el que se actualiza un [[Kubernetes Service]] o un [[Kubernetes Endpoint]] hasta que el kube-dns recibe la notificacion y actualiza sus registros puede que haya informaci\u00f3n erronea en el DNS","title":"Como funciona el DNS server"},{"location":"kubernetes/architecture/Arquitectura%20de%20Kubernetes/#ejemplo-de-como-funciona-todo-junto","text":"Todo kubernetes esta formado por peque\u00f1os elementos muy poco acoplados con una buena separacion de responsabilidades ( cada uno hace una cosa y solo una puta cosa) y esa es la raz\u00f3n por la que todo funciona bien Vamos a ver como funcionan todas las piezas en armon\u00eda para la creacion de un Pod. Normalmente no se crea un Pod desde cero, de modo que vamos a ver como se realiza la creacion de un Pod, pero partiendo del manifiesto de un Deployment. Antes siquiera de empezar ya tenemos la siguiente figura, donde cada uno de los componentes est\u00e1 escuchando las notificaciones adecuadas del API server respecto a los recursos que le son interesantes ( normalmente solo uno o un par a lo sumo) ![[KubernetesWatchingResources.png]] Bien, una vez que kubectl en via mediante un HTTP POST el manifiesto del deployment al API Server esta es la cadena de acontecimientos que se desata. ![[KubernetesPodCreationChainAction.png]] Se pueden ver todos los eventos que van sucediendo en el cluster mediante el siguiente comando de kubectl kubectl get events --watch","title":"Ejemplo de como funciona todo junto"},{"location":"kubernetes/architecture/Arquitectura%20de%20Kubernetes/#entendiendo-que-pasa-cuando-se-ejecuta-un-pod","text":"Cuando ejecutamos un Pod en uno de los nodos no solo se ejecutan los containers que tenemos definidos en el pod, si no que ademas se ejecuta otro pod. Supongamos que solo tenemos un container en el pod para hacerlo mas sencillo. $ kubectl run nginx --image=nginx deployment \"nginx\" created Si vamos al nodo en el que se est\u00e1 ejecutando el pod podemos ver los siguientes contenedores ejecutandose. docker@minikubeVM:~$ docker ps CONTAINER ID IMAGE COMMAND CREATED c917a6f3c3f7 nginx \"nginx -g 'daemon off\" 4 seconds ago 98b8bf797174 gcr.io/.../pause:3.0 \"/pause\" 7 seconds ago Podemos ver que hay un contenedor adicional ejecutandose - Dicho contenedor no est\u00e1 ejecutando nada porque esta en pause - Ademas podemos ver que se ha creado antes que el contenedor que teniamos definido en el pod Ese contenedor es parte de la infraestructura - Todos los contenedores de un mismo pod comparten algunos linux namespaces como por ejemplo el espacio de red ... - Este contenedor es el que contiene todos los linux namespaces que son compartidos por todos los contenedores de un mismo pod. ![[KubernetesPodInfraContainer.png]] Todos los contenedores del mismo pod tienen que compartir los mismos linux namespaces aunque se reinicien, pero eso es posible porque estos est\u00e1n en este container, de manera que aunque los dem\u00e1s se reinicien los namespaces se mantienen. El ciclo de vida de este contenedor esta ligado intimamente al del pod. Si por cualquier motivo este container resultase parado, kubelet volver\u00eda a recrearlo y despu\u00e9s recrear\u00eda todos los demas containers del pod para poder asignarles los linux namespaces del nuevo container.","title":"Entendiendo que pasa cuando se ejecuta un POD"},{"location":"kubernetes/architecture/Arquitectura%20de%20Kubernetes/#interpod-networking","text":"Cada pod tiene un IP unica Todos los pods pueden comunicarse entre si mediante conexiones \"planas\" es decir sin NAT Kubernetes no es quien configura esto, es el administrador quien tiene que conseguirlo. Kubernetes solo pone el requisito. La red tiene que estar configurada de tal manera que la IP que un Pod ve de si mismo sea la misma que los dem\u00e1s pueden ver de \u00e9l. Todos los containers deben de poder comunicarse entre ellos independientemente de si estan siendo ejecutados en el mismo o en distintos nodos.","title":"Interpod networking"},{"location":"kubernetes/architecture/Kubernetes%20Default%20Scheduler%20High%20level%20dynamic%20architecture/","text":"Kubernetes Default Scheduler High level dynamic architecture \u00b6 intro \u00b6 Normalmente no especificamos en qu\u00e9 nodo queremos que se ejecute un Pod. Esto es la tarea del Scheduler. Basicamente la tarea del Scheduler es - recibir las notificaciones de la creacion de nuevos Pods mediante el mecanismo de observer del API server - Modifica el manifiesto para asignarle un worker node donde ejecutarse - Y Vuelve a escribir el manifiesto en el ETCD a traves del API Server. El Scheduler no habla para nada con el Kubelet del nodo El Kubelet es avisado de nuevo por el mecanismo de notificaciones del API Server de que se ha cambiado un recurso ( el Pod en este caso) EL Kubelet lee el nuevo recurso y ve que est\u00e1 asignado a su nodo y por tanto crea el nuevo pod y ejecuta sus containers ( hablando con el runtime que sea Docker o el que haya ) La parte complicada del Scheduler es la parte del algoritmo de decision sobre a que nodo asignar cada nuevo Pod. El algoritmo podr\u00eda ser tan tonto como hacerlo aleatoriamente o tan listo como utilizar Machine Learning para anticipar los nodos que se pueden crear ... etc etc El algoritmo por defecto esta en medio de estos dos. Entendiendo el Algoritmo por defecto del Scheduler. \u00b6 Se pueden identificar dos partes principales: - Filtrar todos los nodos que son ACEPTABLES para poder tener el Pod - Puntuar cada uno de esos nodos segun su prioridad y elegir aquel que tenga una mejor puntuacion para asignarle el pod - Si varios tienen la misma puntuacion intenta elegir mediante un Round-Robin para poder balancear la asignacion de nodos. Fase 1: Encontrando los nodos aceptables \u00b6 Para encontrar los nodos aceptables el Scheduler aplica a cada nodo una lista de predicados que miran distintas cosas como - Puede el nodo cumplir los request del pod? - Esta el nodo bajo de recursos? (reporting memory/disk presure condition) - A pedido el Pod ser asignado a un nodo determinado ( por nombre) - Tiene el nodo una label que corresponda con el node selector del Pod? - Si el Pod pide ser enlazado con un puerto determinado ... est\u00e1 ya ese puerto en uso en el nodo? - Si el Pod pide cargar un determinado volumen ... - Se puede cargar dicho volumen desde el nodo? - Hay algun nodo que ya este usando dicho volumen? - Tolera el Pod los taints del nodo? ( taints and tolerations ) - Especifica el Pod alguna afinidad o antiafinidad por algun nodo o por algun otro pod? - Basicamente podemos querer que determinados pods no esten en el mismo nodo - o Que determinados pods vayan juntos siempre que sea posible ... TODAS estos predicados deben de pasar para que el nodo pase a la lista de aceptables. Una vez pasados todos los nodos por todos los predicados el Scheduler tiene una lista de nodos aceptables. Cualquiera de estos nodos puede ejecutar el pod porque tiene suficientes recursos y cumple los requisitos del pod. Fase 2: Elegir el mejor de los aceptables. \u00b6 Elegir de entre los aceptables no es sencillo porque basicamente no hay un criterio que sea valido para todas las ocasiones. Imagina que tienes un cluster con dos nodos. Ambos nodos son aceptables para ejecutar un pod. Uno de ellos esta ejecutando 10 pods mientras que el otro no tiene ninguno. Parece sencillo, el segundo nodo es la elecci\u00f3n clara porque tiene menos carga, verdad? O puede que **NO**. Si estas ejecutando en un entorno cloud puede que quieras ejecutar el nuevo pod en el nodo uno ( que tiene los otros 10) de manera que puedas liberar el nodo que no usas y ahorrar pasta. Como vemos no hay una sola manera de hacer las cosas. Por defecto los pods que pertenecen al mismo [[Kubernetes Service|Service]] o [[Kubernetes ReplicaSet|ReplicaSet]] se intentan mandar a distintos nodos, aunque esto no est\u00e1 garantizado. Se pueden definiri reglas de affinity o anti-affinity para configurar todo esto. Usando multiples Schedulers. \u00b6 Se pueden lanzar multiples Schedulers en el cluster. Luego se elige que Scheduler maneja cada pod con la propiedad del pod: spec.schedulerName Los que no tengan nada o tengan default-scheduler como valor, ser\u00e1n tratados por el Scheduler por defecto.","title":"Kubernetes Default Scheduler High level dynamic architecture"},{"location":"kubernetes/architecture/Kubernetes%20Default%20Scheduler%20High%20level%20dynamic%20architecture/#kubernetes-default-scheduler-high-level-dynamic-architecture","text":"","title":"Kubernetes Default Scheduler High level dynamic architecture"},{"location":"kubernetes/architecture/Kubernetes%20Default%20Scheduler%20High%20level%20dynamic%20architecture/#intro","text":"Normalmente no especificamos en qu\u00e9 nodo queremos que se ejecute un Pod. Esto es la tarea del Scheduler. Basicamente la tarea del Scheduler es - recibir las notificaciones de la creacion de nuevos Pods mediante el mecanismo de observer del API server - Modifica el manifiesto para asignarle un worker node donde ejecutarse - Y Vuelve a escribir el manifiesto en el ETCD a traves del API Server. El Scheduler no habla para nada con el Kubelet del nodo El Kubelet es avisado de nuevo por el mecanismo de notificaciones del API Server de que se ha cambiado un recurso ( el Pod en este caso) EL Kubelet lee el nuevo recurso y ve que est\u00e1 asignado a su nodo y por tanto crea el nuevo pod y ejecuta sus containers ( hablando con el runtime que sea Docker o el que haya ) La parte complicada del Scheduler es la parte del algoritmo de decision sobre a que nodo asignar cada nuevo Pod. El algoritmo podr\u00eda ser tan tonto como hacerlo aleatoriamente o tan listo como utilizar Machine Learning para anticipar los nodos que se pueden crear ... etc etc El algoritmo por defecto esta en medio de estos dos.","title":"intro"},{"location":"kubernetes/architecture/Kubernetes%20Default%20Scheduler%20High%20level%20dynamic%20architecture/#entendiendo-el-algoritmo-por-defecto-del-scheduler","text":"Se pueden identificar dos partes principales: - Filtrar todos los nodos que son ACEPTABLES para poder tener el Pod - Puntuar cada uno de esos nodos segun su prioridad y elegir aquel que tenga una mejor puntuacion para asignarle el pod - Si varios tienen la misma puntuacion intenta elegir mediante un Round-Robin para poder balancear la asignacion de nodos.","title":"Entendiendo el Algoritmo por defecto del Scheduler."},{"location":"kubernetes/architecture/Kubernetes%20Default%20Scheduler%20High%20level%20dynamic%20architecture/#fase-1-encontrando-los-nodos-aceptables","text":"Para encontrar los nodos aceptables el Scheduler aplica a cada nodo una lista de predicados que miran distintas cosas como - Puede el nodo cumplir los request del pod? - Esta el nodo bajo de recursos? (reporting memory/disk presure condition) - A pedido el Pod ser asignado a un nodo determinado ( por nombre) - Tiene el nodo una label que corresponda con el node selector del Pod? - Si el Pod pide ser enlazado con un puerto determinado ... est\u00e1 ya ese puerto en uso en el nodo? - Si el Pod pide cargar un determinado volumen ... - Se puede cargar dicho volumen desde el nodo? - Hay algun nodo que ya este usando dicho volumen? - Tolera el Pod los taints del nodo? ( taints and tolerations ) - Especifica el Pod alguna afinidad o antiafinidad por algun nodo o por algun otro pod? - Basicamente podemos querer que determinados pods no esten en el mismo nodo - o Que determinados pods vayan juntos siempre que sea posible ... TODAS estos predicados deben de pasar para que el nodo pase a la lista de aceptables. Una vez pasados todos los nodos por todos los predicados el Scheduler tiene una lista de nodos aceptables. Cualquiera de estos nodos puede ejecutar el pod porque tiene suficientes recursos y cumple los requisitos del pod.","title":"Fase 1: Encontrando los nodos aceptables"},{"location":"kubernetes/architecture/Kubernetes%20Default%20Scheduler%20High%20level%20dynamic%20architecture/#fase-2-elegir-el-mejor-de-los-aceptables","text":"Elegir de entre los aceptables no es sencillo porque basicamente no hay un criterio que sea valido para todas las ocasiones. Imagina que tienes un cluster con dos nodos. Ambos nodos son aceptables para ejecutar un pod. Uno de ellos esta ejecutando 10 pods mientras que el otro no tiene ninguno. Parece sencillo, el segundo nodo es la elecci\u00f3n clara porque tiene menos carga, verdad? O puede que **NO**. Si estas ejecutando en un entorno cloud puede que quieras ejecutar el nuevo pod en el nodo uno ( que tiene los otros 10) de manera que puedas liberar el nodo que no usas y ahorrar pasta. Como vemos no hay una sola manera de hacer las cosas. Por defecto los pods que pertenecen al mismo [[Kubernetes Service|Service]] o [[Kubernetes ReplicaSet|ReplicaSet]] se intentan mandar a distintos nodos, aunque esto no est\u00e1 garantizado. Se pueden definiri reglas de affinity o anti-affinity para configurar todo esto.","title":"Fase 2: Elegir el mejor de los aceptables."},{"location":"kubernetes/architecture/Kubernetes%20Default%20Scheduler%20High%20level%20dynamic%20architecture/#usando-multiples-schedulers","text":"Se pueden lanzar multiples Schedulers en el cluster. Luego se elige que Scheduler maneja cada pod con la propiedad del pod: spec.schedulerName Los que no tengan nada o tengan default-scheduler como valor, ser\u00e1n tratados por el Scheduler por defecto.","title":"Usando multiples Schedulers."}]}